---
title: "Multiple Linear Regression"
author: "Manoel Galdino"
date: "2024-01-26"
output: ioslides_presentation
---
## Agenda for today's lecture

- **Objectives of the Lecture:** 

-- Learn how to use the matrix representation of regression

-- Lear how to perform multiple regressions

-- Learn how to interpret multiple regression

## Multiple Regression

- Multiple regression is about using more than one predictor.
- Most of the lessons learned still work.

- To understand it, it's easier with matrix, instead of summation.

- The derivations are easier as well.

# Review of basic vector and matrix operations

## Overview of Vectors

Vectors are one-dimensional arrays of numbers, fundamental in representing data in multiple regression.

- **Definition**: A vector in R can be created using the `c()` function. 
  - Example: `c(1, 2, 3)` creates a vector containing 1, 2, and 3.
- **Sum of Vectors**: The sum of two vectors is performed element-wise.
  - For `u = (u1, u2)` and `v = (v1, v2)`, the sum is `u + v = (u1 + v1, u2 + v2)`.
- **Multiplication by a Scalar**: Multiplying a vector by a scalar multiplies each element by that scalar.
  - For scalar `a` and vector `v`, the product is `a * v`.

## Overview of Vectors - R

```{r vector-example, echo=TRUE}
# Example in R
u <- c(1, 2)
v <- c(3, 4)
u + v  # Sum of vectors
2 * u  # Multiplication by a scalar
```

## Overview of Matrices

Matrices are two-dimensional arrays of numbers, which are especially useful in multiple regression for handling multiple variables simultaneously.

-A matrix in R is created using the `matrix()` function. 
- Example: `matrix(1:6, nrow=2, ncol=3)` creates a 2x3 matrix.

- **Sum of Matrices**: The sum is also performed element-wise. 
  - If `A` and `B` are matrices, then their sum `C = A + B` has elements `c_ij = a_ij + b_ij`.
  
- **Matrix Multiplication**: Matrix multiplication is more complex. The product of an `m x n` matrix `A` and an `n x p` matrix `B` is an `m x p` matrix `C`, where each element `c_ij` is the sum of the products of corresponding elements from the ith row of `A` and the jth column of `B`.
  
## Overview of Matrices (cont.)

```{r matrix-example, echo=TRUE}
# Example in R
A <- matrix(1:4, nrow=2)
B <- matrix(5:8, nrow=2)
# Sum of matrices
A + B
# Matrix multiplication
A %*% B
```

## Multiple Linear Regression Model - Overview

The basic multiple linear regression model can be described as follows:

- **Predictors**: There are \( p \) predictors, denoted as \( X_1, X_2, \ldots, X_p \).
  - No assumptions are made about the distribution of these predictors. They may be correlated or not.

- **Response Variable**: 
  - There is a single response variable, \( Y \).
  - In the case of multiple response variables, the model would be a multivariate regression model.

## Regression Equation

The regression model is specified as:

\[ y_i = \alpha + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots + \beta_p x_{pi} + e_i \]

- We have \( p + 1 \) parameters or regression coefficients to estimate: the intercept \( \alpha \) and the slopes \( \beta_1, \beta_2, \ldots, \beta_p \).

## Error Term Characteristics

- The error term \( e_i \) in the model:
  - Has a conditional expectation of zero.
  - Possesses a constant conditional variance in the homoscedastic model.
  - Is uncorrelated between observations.

## Normality Assumption of the Error Term

If normality of the error term is assumed:

- The error \( e_i \) follows a multivariate normal distribution.
  - Mean vector is zero.
  - Variance-covariance matrix has zero off-diagonal elements (covariance).
  - Main diagonal elements are \( \sigma^2 \), representing the variance.

# Matrix Representation of Regression

## Matrix Representation - Introduction

In a matrix representation, the multiple linear regression model is efficiently described. This representation is key for understanding the model's linear algebra foundations.

## Response Vector \( \mathbf{y} \)

The response variable \( Y \) for \( n \) observations is represented as a column vector \( \mathbf{y} \) of dimension \( n \times 1 \):

\[ \mathbf{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} \]

This vector contains the observed values of the response variable for each observation.

## Design Matrix \( \mathbf{X} \)

The design matrix \( \mathbf{X} \) contains the predictor values and is of dimension \( n \times (p+1) \):

\[ \mathbf{X} = \begin{bmatrix} 
1 & x_{11} & x_{12} & \cdots & x_{1p} \\
1 & x_{21} & x_{22} & \cdots & x_{2p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \cdots & x_{np} \\
\end{bmatrix} \]

The first column is typically a column of ones for the intercept, followed by columns for each predictor.

## Coefficient Vector \( \boldsymbol{\beta} \)

The regression coefficients, including the intercept, are represented as:

\[ \boldsymbol{\beta} = \begin{bmatrix} \alpha \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_p \end{bmatrix} \]

This vector is of dimension \( (p+1) \times 1 \).

## Error Vector \( \mathbf{e} \)

The error terms are represented as a column vector \( \mathbf{e} \) of dimension \( n \times 1 \):

\[ \mathbf{e} = \begin{bmatrix} e_1 \\ e_2 \\ \vdots \\ e_n \end{bmatrix} \]

Each element represents the error for the corresponding observation.

## Matrix Equation

The multiple linear regression model in matrix form is given by:

\[ \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{e} \]

This equation captures the relationship between the response variable and multiple predictors.

# Assumptions of the Model


## Expectation of Error

The expectation of the error term, given the predictors, is zero:

\[ \mathbb{E}[\epsilon | X] = 0 \]

This implies that the error term does not systematically vary with the predictors.

## Variance of Error

The variance of the error term, given the predictors, is constant and can be represented as:

\[ \text{Var}[\epsilon | X] = \sigma^2 \mathbf{I} \]

where \( \mathbf{I} \) is the identity matrix. 

## Identity Matrix \( \mathbf{I} \)

The identity matrix, \( \mathbf{I} \), is a special matrix with ones on the main diagonal and zeros elsewhere:

\[ \mathbf{I} = \begin{bmatrix}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{bmatrix} \]

This structure ensures homoskedasticity, meaning the error variance is the same across all observations.

## Interpretation of Regression Coefficients

Understanding the interpretation of regression coefficients is crucial in multiple linear regression analysis. This presentation covers different scenarios.

## 1. Independent Predictors

When no predictor is a function of another predictor, each coefficient represents the change in the response variable for a one-unit change in the predictor, holding all other predictors constant.

- **Example**: If \(\beta_1\) is the coefficient for \(X_1\), a one-unit increase in \(X_1\) is associated with a \(\beta_1\) unit change in \(Y\), assuming other predictors remain unchanged.

## 2. Predictors as Functions of Other Predictors

When some predictors are functions of other predictors, the interpretation becomes more complex due to multicollinearity. The effect of one predictor depends on the level of the other predictor it's related to.

- **Implication**: Careful statistical analysis and possibly dimensionality reduction techniques are needed to understand the individual contributions of predictors.

## 3. Interaction Terms

When there is an interaction term between predictors, the effect of one predictor on the response variable depends on the value of another predictor.

- **Example**: If we have an interaction term \(\beta_3 X_1 X_2\), the effect of \(X_1\) on \(Y\) changes as \(X_2\) changes, and vice versa. This allows for non-additive effects between predictors (non-linear model in variables).

## 4. Categorical Predictors (Binary)

With binary (0/1) categorical predictors, the coefficient represents the difference in the response variable's mean value when the predictor changes from 0 to 1, holding all other predictors constant.

- **Example**: If \(\beta_4\) is the coefficient for a binary predictor \(X_4\) (where 1 represents the presence of a characteristic and 0 its absence), \(\beta_4\) represents the mean change in \(Y\) associated with the presence of this characteristic.

## Summary

- **Independent Predictors**: Coefficients show the isolated effect of each predictor.
- **Functionally Related Predictors**: Coefficients must be interpreted with caution due to correlation among them.
- **Interaction Terms**: Coefficients indicate how the effect of one predictor on the response changes with another predictor.
- **Categorical Predictors**: Coefficients represent the mean difference in response between categories.

#  Omitted Variable Bias

## Multiple Regression vs. Separate Simple Regressions:

Running a regression with two variables (say) is not the same as running two separate regressions, one for each variable.

This difference is due to the general correlation that exists between predictors. 

To understand this, assume the true model is $Y = \alpha + \beta_1 \cdot X_1 + \beta_2 \cdot X_2 + \epsilon$.

What would happen if we run a regression with just one predictor ($X_1$)?

## Properties of Covariance

We'll use the following properties of covariance in this derivation:

1. Let $X$ and $Y$ be two random variables, and $A$ a constant. Then, $\mathbb{Cov}[X, Y + A] = \mathbb{Cov}[X, Y] + \mathbb{Cov}[X, A]$. Since $A$ is constant, $\mathbb{Cov}[X, A] = 0$, hence $\mathbb{Cov}[X, Y + A] = \mathbb{Cov}[X, Y]$.

2. $\mathbb{Cov}[X, Y \cdot A] = A \cdot \mathbb{Cov}[X, Y]$.

## Single Predictor Model

In a model with a single predictor, we have:
$Y = \alpha + \beta_1^* \cdot X_1 + \epsilon$

Here, $\beta_1^*$ is designated for the equation with one predictor, to differentiate it from $\beta_1$ in the true equation with two predictors.

We know that $\beta_1^* = \frac{\mathbb{Cov}[X_1, Y]}{\mathbb{Var}[X_1]}$

Let's substitute the $Y$ from the true model into the equation for $\beta_1^*$.

## Deriving $\beta_1^*$

$$
\begin{align*}
\beta_1^* = \frac{\mathbb{Cov}[X_1, \alpha + \beta_1 \cdot X_1 + \beta_2 \cdot X_2 + e]}{\mathbb{Var}[X_1]} \\
= \frac{\beta_1 \cdot \mathbb{Cov}[X_1, X_1] + \beta_2 \cdot \mathbb{Cov}[X_1, X_2]}{\mathbb{Var}[X_1]} \\
= \beta_1 + \frac{\beta_2 \cdot \mathbb{Cov}[X_1, X_2]}{\mathbb{Var}[X_1]}
\end{align*}
$$

Thus, the slope $\beta_1^*$ includes the direct contribution of $X_1$ via $\beta_1$ and the indirect contribution from its correlation with $X_2$, via $\beta_2$.

## Implications and Omitted Variable Bias

Running a regression with one predictor when the true model has two predictors means that the coefficient $\beta_1^*$ will be a blend of $\beta_1$ and $\beta_2$.

Conversely, with the correct model using both predictors, $\beta_1^*$ will reflect only the contribution of $x_1$.

One might wonder: what if the true model has more than two predictors? This leads to the concept of omitted variable bias. If we omit a variable $X_k$ correlated with $X_j$ ($j \neq k$) from the regression, the coefficient $\beta_j$ will also reflect the effect of $\beta_k$.

## Causal Inference and Modern Approaches

This discussion isn't about causality, but rather about the contribution to predicting our response variable.

Before modern approaches like Rubin's potential outcomes framework or Pearl's Bayesian network models, controlling for as many variables as possible was thought to potentially eliminate omitted variable bias, thereby ensuring that $\beta_1$ does estimate the causal effect.

Today, we understand that the most reliable way to think about causality is using one of these two approaches, and checking (e.g., with potential outcomes) that the Conditional Independence Assumption (CIA) is plausible for causal interpretation of $\beta_1$.

Without a causal model, merely introducing regressors to control for omitted variable bias doesn't allow causal inference, except in very simple cases or when CIA is implicitly guaranteed.

## Multicollinearity

So far, we have not discussed the conditions under which the inverse matrix $X'X^{-1}$ exists.

We know that not all matrices are invertible. Matrices with a zero determinant are non-invertible, similar to the concept that we cannot divide a scalar by zero. 

The determinant is zero when columns are not linearly independent, i.e., when one column (or more) is a linear combination of other columns. In our context, this happens when the correlation is $1$ (or $-1$), leading to multicollinearity.

Modern software, like R, will automatically drop one or more variables if this occurs, to ensure that the matrix is invertible. Thus, unless the correlation is perfect, multicollinearity is usually not a problem.


## Robust Standard Error (toy model)

In the presence of heteroscedasticity or correlation in errors (such as temporal or spatial autocorrelation), we need to adjust the calculation of the standard error. Let's see with a simple toy simulation how this problem can occur.

```{r se-robusto toy plot, message=FALSE, warning=FALSE}
library(ggplot2)
library(tidyverse)
set.seed(123)

x <- c(1:8, 10, 15)
y <- c(5 + rnorm(8, sd = 1.5), 40, 65)
df <- data.frame(y=y, x=x)


```
## Robust Standard Error (toy model cont. 1)

```{r se-robusto toy fit, message=FALSE, warning=FALSE, echo=FALSE}
df %>%
  ggplot(aes(x=x, y=y)) + geom_point()
fit <- lm(y ~ x)
```

## Robust Standard Error (toy model cont. 2)

```{r se-robusto toy print, message=FALSE}
summary(fit)
```

## Robust Standard Error - HC1
In our "toy model", it doesn't make sense to assume constant variance. It's likely that the two outliers came from a distribution with much larger variance, leading to a larger standard error for the coefficient of variable $x$. 

The question then is how to calculate different variances for these points. There are numerous ways to do this. The default in Stata, called "HC1" (Heteroscedasticity-Consistent), has the following formula for the robust standard error "HC1":

$$
\frac{n}{n-k}\hat{e}^2
$$

where $\hat{e}^2$ is the squared residuals and $k$ is the number of parameters. In our simple model, $k=2$.



## Robust Standard Error - HC3

The robust standard error in the R package "sandwich" uses "HC3", which has a different formula and performs better in small samples. Its formula is:

$$
\frac{\hat{e}^2}{(1-h_i)^2}
$$

## Robust Standard Error in R

```{r se-sandwich hc3, message=FALSE, warning=FALSE}
library(lmtest)
library(sandwich)
coeftest(fit, vcovHC(fit, "HC3"))
```

## Checking for Outliers

One way to check for this issue is to look for large residuals and highly influential observations (high leverage values). A basic command in R allows us to visually inspect if this is the case. Points in the top right or bottom right corners indicate observations exerting influence in our model.

Large residuals (or non-constant variance) may result from a poorly specified model (missing predictor, absence of interaction, nonlinear effects, etc.).

## Robust standard errors - summary

Robust standard errors are essential when dealing with heteroscedasticity or autocorrelation in errors.

There are many types of robust SEs.

- **HC0, HC1, HC2, HC3:** 
- **HC1 (Heteroscedasticity-Consistent):** Commonly used, adjusts for heteroscedasticity.
- **HC3:** Preferred in small samples, adjusts the squared residuals based on leverage.

Use of `lmtest` and `sandwich` packages in R for hypothesis testing with robust standard errors, accommodating for outliers and heteroscedasticity.

**Detecting Influential Observations:** Large residuals or high leverage points can indicate potential issues.
