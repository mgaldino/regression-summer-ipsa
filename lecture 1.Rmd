---
title: "Linear Regression - estimation"
author: "Manoel Galdino"
date: "2024-01-26"
output:
  xaringan::moon_reader:
    self_contained: true
    seal: false
    nature:
      highlightStyle: github
      slideNumber: true
      ratio: "16:9"
---

## Agenda for today's lecture

- **Objective of the Lecture:** 

-- Understand the assumptions of the linear regression model

-- Understand the implications of the assumptions of the linear regression model

-- Learn what Ordinary Least Squares (OLS) does and does not.

-- Learn that there is a difference between structural causal model and linear regression model.

---

## Terminology
- There is a standard terminology to refer to $Y$ and $X$ variables:

```{r table-terminology, echo=FALSE, message=FALSE, warning=FALSE}

df <- data.frame(Y = c("Dependent variable", "Response variable", "Explained variable", "Predicted variable", "Regressand"),
                 X = c("Independent variable", "Control variable", "Explanatory variable", "Predictor variable", "Regressor"))

knitr::kable(df, format = "html", caprtion = "Terminology for Simple Regression")
```

- Other terms: "covariate" for $x$ and "outcome" for $y$.

- Sometimes we will just write that we regressed $y$ on $x$.

---

## Assumptions 

- To estimate a model and make inferences, certain assumptions are required.

- It is important to understand that the assumptions for estimation differ from those for inference.

- Let's start with assumptions needed for (good) estimation.

- reminder: in statistics, "good" estimators may refer to consistency (estimator converges to the true), unbiasedness (what is un unbiased estimator?) and efficiency (it has the lowest variance among a given class of estimators, say, unbiased ones).

---

## Good assumptions for a linear model

1. The linear (in the parameters) population model is $y = \alpha + \beta x_i + \epsilon_i$

2. $\mathbb{E}[\epsilon_i] = 0$ (this is a trivial assumptio)

3. The sample data $\{(x_i, y_i): i = 1, 2, ..., n\}$ is a random sample of size $n$ from the population

4. $\mathbb{E}[\epsilon|X=x] = \mathbb{E}[\epsilon]$. Shorthand version is $\mathbb{E}[\epsilon|x] = \mathbb{E}[\epsilon]$

5. The sample variance of $X$ is not zero.

- Using assumption $2$ and $4$ we can show that $\mathbb{E}[\epsilon|X=x] = 0$

- Assumptions 3 and 4 are the most critical assumptions, in particular assumption 4.

- We call it (4) the **zero conditional mean assumption**


---

## Zero conditional mean assumption

- What does this assumption mean?

- Consider that our model estimates the effect of GDP growth on the popularity of a president

- To simplify, let's think of the error term as the charisma of the leader, an unmeasured variable.

- The model is $y_i = \alpha + \beta x_i + \epsilon_i$; with assumption $3$ and the simplification that $\epsilon_i$ is charisma means that:

- $\mathbb{E}[\text{charisma}|gdp=.05] = \mathbb{E}[\text{charisma}|gdp=.02]$ etc. for all values of gdp growth.

- Since we do not observe popularity, we do not know if this assumption is true.

- Another interpretation is given by: $COV(X,\epsilon) = \mathbb{E}[X\epsilon] - \mathbb{E}[X]*\mathbb{E}[\epsilon] = \mathbb{E}[X\epsilon] = \mathbb{E}[\mathbb{E}[X\epsilon|X]] = \mathbb{E}[X \mathbb{E}[\epsilon|X]]= 0$

- Later we will connect this assumption with causality more generally.

- Another useful result is: $\mathbb{E}[Y|X] = \alpha + \beta x_i$. It is called the **population regression function**.

---

## Estimation

- First, for any $\hat{\alpha}$ and $\hat{\beta}$, define the fitted value for y when $x = x_i$ as:
$\hat{y} = \hat{\alpha} + \hat{\beta} x_i$.

- The residual $e_i$ for each observation $i$ is the difference between the observed $y_i$ and its fitted value $\hat{y_i}$: $e_i =  y_i - \hat{y_i} = y_i - (\hat{\alpha} + \hat{\beta} x_i)$.

- the residuals are **not** the same as the errors. The later is a population concept. Residuals are for a given sample and depend on the estimated parameters.

- Error definition: errors are theoretical and unobservable: the amounts by which the observed values $y_i$ differ from the conditional expectation $\mathbb{E}[Y|X]$. A population concept.

- Residuals definition: the residuals are the amounts by which the observed values $y_i$ differ from regression estimates of their expected values $\hat{y_i}$.

---


## Ordinary Least Squares (OLS)

- Choose $\hat{\alpha}$ and $\hat{\beta}$ that minimizes the sum of the squared residuals, i.e, $\sum_{i=1}^{n}(e_i)^2 = \sum_{i=1}^{n}(y_i - (\hat{\alpha} + \hat{\beta} x_i))^2$

- If we take the derivatives with respect to $\alpha$ and $\beta$ and make them equal to zero, we find the values of $\alpha$ and $\beta$ that minimize the above sum.

- The solution to this problem is: 
$\hat{\alpha} = \bar{y} - \hat{\beta}*\bar{x}$ and $\hat{\beta} = C_{X,Y}/S_X^2$

The prediction from the fitted model $\hat{Y}$ is given by $ \bar{y} - \hat{\beta}*\bar{x} +  \frac{C_{X,Y}}{S_X^2}X$

- We call the Ordinary Least Squares (OLS) this estimator for the parameters of the model.

- the OLS estimator assure us that the residuals (not the errors) are also uncorrelated with predictors.

---

## Causality

- A structural (or causal) model in polisci represents the behavioral relationship among variables.

- The parameters represent the true causal effects

- The population linear regression model will be strcutural only if assumption 4 is valid.

- We define causality in terms of potential outcomes

- Let's say we are interested in estimating the effect of an intervention (say, conditional cash transfer, measured as $T$) on some outcome $Y$ (e.g. poverty rate).

- We say that the average causal effect (or Average Treatment Effect) is: $\mathbb{E}[Y(1) - Y(0)]$

- Is possible to show that, if the treatment $T$ is independent of the potential outcomes, than standard regression estimates is an unbiased estimator of the true causal effect. We call it the Independence Assumption (IA). Whenever the IA assumption holds, then the zero mean conditional assumption also holds.

- In a regression with other independent variables, if the  treatment $T$ is independent of the potential outcomes, conditional on the other regressors $X$, then the OLS estimator is unbiased. We call it the Conditional Independence Assumption (CIA). Again, if it holds, the zero mean conditional assumption also holds.

---

## OLS is unbiased

- If we assume that $\mathbb{E}[\epsilon|X] = 0$, then we can prove the estimator is unbiased.

- We will use the following property: $\sum\limits_{i=1}^n{}(x_i - \bar{x})(y_i - \bar{y}) = \sum\limits_{i=1}^n{}(x_i - \bar{x})y_i$

- To prove it, we only need to expand the product, use some summation properties (linearity) and some algebraic manipulation.

- We know that $\hat{\beta} = \frac{C_{X,Y}}{S_X^2} = \frac{\sum\limits_{i=1}^n{}(x_i - \bar{x})(y_i - \bar{y})}{nS_X^2}$

- Thus, we can rewrite it as: $\hat{\beta} = \frac{\sum\limits_{i=1}^n{}(x_i - \bar{x})y_i}{nS_X^2}$

- We are talking about the behavior of $\hat{\beta}$ across infinite samples. In other words, we can treat it as a random variable in itself.

- Substitute $y_i$ by its population model: $\hat{\beta} = \frac{\sum\limits_{i=1}^n{}(x_i - \bar{x}) (\alpha + \beta x_i + \epsilon_i)}{nS_X^2}$

---

## OLS is unbiased II

- Focus on the numerator only

$$\begin{align}
\sum\limits_{i=1}^n{}(x_i - \bar{x}) (\alpha + \beta x_i + \epsilon_i) &= \sum\limits_{i=1}^n{}((x_i - \bar{x})\alpha + (x_i - \bar{x}) \beta x_i + (x_i - \bar{x}) \epsilon_i) \\
&= \alpha\sum\limits_{i=1}^n{}(x_i - \bar{x}) + \beta \sum\limits_{i=1}^n{}(x_i - \bar{x})x_i + \sum\limits_{i=1}^n{}(x_i - \bar{x})\epsilon_i  \\
\end{align}$$

- We know that $\sum\limits_{i=1}^n{}(x_i - \bar{x}) = 0$ and that $\sum\limits_{i=1}^n{}(x_i - \bar{x})x_i = \sum\limits_{i=1}^n{}(x_i - \bar{x})(x_i - \bar{x})$.

- Thus we can write our equation as: $\hat{\beta} = \beta + \frac{\sum\limits_{i=1}^n{}(x_i - \bar{x})\epsilon_i}{nS_x^2}$

- If we condition of $x$, then we can write:

$$\begin{align}
\mathbb{E}[\hat{\beta}|X] = \beta + \mathbb{E}[\frac{\sum\limits_{i=1}^n{}(x_i - \bar{x})\epsilon_i}{nS_x^2}|X] = \beta +\frac{\sum\limits_{i=1}^n{}\mathbb{E}[(x_i - \bar{x})\epsilon_i|X]}{nS_x^2} = \beta +\frac{\sum\limits_{i=1}^n{}(x_i - \bar{x})\mathbb{E}[\epsilon_i|X]}{nS_x^2} = \beta
\end{align}$$

- As expected.

--- 

## Variance of OLS

Lets explore the variance of the beta estimator:

$$
\mathbb{Var}[\hat{\beta}] = \mathbb{Var} \left( \frac{\beta +  \sum({x - \bar{x}})e_i}{nS_x^2} \right)
$$

- We know that $\mathbb{Var}[x + a] = \mathbb{Var}[x]$. Hence, we can eliminate $\beta$ from the equation:

$$
\mathbb{Var}[\hat{\beta}] = \mathbb{Var} \left( \frac{\sum({x - \bar{x}})e_i}{nS_x^2} \right)
$$

- Since all observations of $x$ have been observed, the sum of differences from the mean is a constant. We also know that $\mathbb{Var}[x*a] = a^2\mathbb{Var[x]}$.

---

## Variance of OLS (cont. 2)

$$\begin{align*}
\mathbb{Var}[\hat{\beta}] = \frac{\sum({x - \bar{x}})^2}{n^2S_x^4} \mathbb{Var}[e_i] = \\
\frac{nS_x^2}{n^2S_x^4}\mathbb{Var}[e_i] = \\
\frac{1}{nS_x^2}\mathbb{Var}[e_i]
\end{align*}$$

If we assume that $\mathbb{Var}[e_i] = \sigma^2$, then:

$$
\mathbb{Var}[\hat{\beta}] = \frac{\sigma^2}{nS_x^2}
$$
- This assumption is called homoscedasticity.


---

## Variance of OLS (cont. 3)

- The variance of our estimator is the variance of the error divided by the variance of $x$ multiplied by the sample size. 

- This implies that: when $Y$ is more dispersed, our estimator has higher variance.

- A larger sample size leads to lower variance. 

- When $X$ is more spread out, it results in a lower estimator variance.

---

## Standard Error Calculation

Finally, the standard error of an estimate is simply the square root of the variance. Therefore:

$$
se(\hat{\beta}) = \frac{\sigma}{\sqrt{nS^2_x}}
$$

This equation represents the standard error of our beta estimator, showing how it depends on the error standard deviation, sample size, and variance of $x$.

