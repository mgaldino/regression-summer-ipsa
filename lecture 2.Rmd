---
title: "Linear Regression - MLE and Inference"
author: "Manoel Galdino"
date: "2024-01-26"
output:
  xaringan::moon_reader:
    self_contained: true
    seal: false
    nature:
      highlightStyle: github
      slideNumber: true
      ratio: "16:9"
---

## Agenda for today's lecture

- **Objective of the Lecture:** 

-- Learn what is the Maximum Likelihood Estimator (MLE)

-- Learn how to make inference with MLE

-- Learn how to make inference using the $t$ distribution

-- Learn what is the implication of the Normal errors assumption

---

## Maximum Likelihood

- We've discussed several important aspects of OLS, including the unbiasedness and variance of $\hat{\beta}$.
- However, OLS does not provide any sampling distributions.
- As a result, we cannot make inferences such as hypothesis testing or constructing confidence intervals.
- Therefore, we will introduce the method of maximum likelihood for simple linear regression.

---

## Maximum Likelihood - assumptions

- There are no specific assumptions about X.
- We continue to use a linear model to approximate the Conditional Expectation Function (CEF).
- We will assume that the error term $e$ is normally distributed as $N(0, \sigma^2)$ and is independent of $X$.
- Error terms are independent across observations.

---

## Maximum Likelihood vs OLS - assumptions

- What has changed?
- The first two assumptions remain the same.
- The third assumption is new. Previously, we only required the errors to have a zero mean (and, for the variance derivation, a constant variance). Now, we are assuming a specific distribution for the errors (Gaussian).
- We have moved from assuming uncorrelatedness to independence, which is a stronger condition.

---

## Maximum Likelihood - what do we get?

- "With great power comes great responsibility."
- Additional assumptions grant more power. This enables us to write a probability distribution for $Y$.
- $y_i = \alpha + \beta x_i + e_i$, where $e_i \sim N(0, \sigma^2)$.
- Alternative parametrization: $y_i \sim N(\mu, \sigma^2)$. Here, $\mu = \alpha + \beta x_i$.
- The pdf of $Y$ given $X=x$ is $p(y|X=x; \alpha, \beta, \sigma^2)$. This notation highlights the distinction between observables and parameters.

---

## Maximum Likelihood - what do we get?

<style>
.small-font {
    font-size: 80%;
}
</style>

<div class="small-font">
- For any given data set, we can express the probability density, under our model, of observing that specific data.
$$
\begin{align*}
\prod_{i=1}^n p(y_i|x_i;\alpha, \beta, \sigma^2) = \\
\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y_i - \mu)^2}{2\sigma^2}\right) = \\
\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y_i - (\alpha + \beta x_i))^2}{2\sigma^2}\right)
\end{align*}
$$
<div>

---

- After collecting the data, with some estimated parameters, the equation becomes:

$$
\prod_{i=1}^n \frac{1}{\sqrt{2\pi S^2}}\exp\left(-\frac{(y_i - \hat{\alpha} - \hat{\beta} x_i)^2}{2S^2}\right)
$$
- This expression represents the likelihood, a function of the estimated parameters.

---

## Maximum Likelihood Estimation

- Our goal is to select parameter values that maximize the probability of observing the data.
- Utilizing calculus and working with the log-likelihood (applying the logarithm to the function alters the function but not the location of its maximum), we can easily derive the formulas for $\hat{\alpha}$ and $\hat{\beta}$ that maximize the likelihood.
- $\hat{\alpha} = \bar{y} - \hat{\beta} \bar{x}$
- $\hat{\beta} = \frac{\text{Cov}(X,Y)}{S^2_x}$
- $\hat{\sigma}^2 = \mathbb{E}[(y_i - \hat{\alpha} - \hat{\beta}x_i)^2]$
- We obtain the same estimators as before, plus an estimator for $\sigma^2$

---

## MLE - Sampling Distributions

- Recall from previous classes: $\hat{\beta} = \beta + \frac{\sum_{i=1}^n (x_i - \bar{x})e_i}{nS^2_x}$
- We know that if $X \sim N(\mu_x, \sigma^2_x)$ and $Y \sim N(\mu_y, \sigma^2_y)$ are independent, then for $Z = X + Y$, we have $Z \sim N(\mu_x + \mu_y, \sigma^2_x + \sigma^2_y)$.
- Using the fact that error term is distributed as $e_i \sim N(0, \sigma^2)$.
- And with errors being independent Gaussians, $\hat{\beta}$ is also Gaussian, we can get the sampling distribution of $\hat{\beta}$.
- Knowing its mean and variance, we can state:
- $\hat{\beta} \sim N(\beta, \frac{\sigma^2}{nS^2_x})$


---

## Properties and limitations of MLE

- MLE estimators are **asymptotically efficient**, meaning their parameter estimates converge on the truth "as quickly as possible" as $n$ grows.
- They are also unbiased, like the OLS estimators.
- However, if the error is not Gaussian distributed, then the sampling distribution calculations we just did are incorrect. Consequently, our hypothesis tests, confidence intervals, etc., will also be incorrect.
- How incorrect?

---

## Simulation of a wrong assumption

- If you want to understand some idea, it's alwyas useful to run a simulation

- Let's explore this approach in the case of normal errors

- What if my true error distribution is not normal?

- Let's run four simulations where: 1. errors are normal; 2. errors are t-distributed; 3. errors are Beta distributed; 4. errors are a mixture of Normals.

- In every case, there is a fixed part of the DGP: $\mu = \alpha + \beta x_i$, with $\alpha = 5$ and $\beta = 1$, $x \sim N(2, 4)$. And the part that we change from each simulation is the distribution of the error term.

---

# Inference

## Sampling Distribution

- We know that $\hat{\beta} \sim N(\beta, \frac{\sigma^2}{nS^2_x})$.

- To make inferences, we need to know the true $\beta$ and the error variance, which we don't. Therefore, we manipulate the equation to remove one of the parameters of the probability distribution.

- $\hat{\beta} - \beta \sim N(0, \frac{\sigma^2}{nS^2_x})$.

- We still have one unknown in the probability distribution. As a result, we need to estimate $\sigma^2$.

---

## Sampling Distribution of the estimator of $\sigmaË†2$.

- The estimator of the variance can be defined as the sum of the squared residuals (since the mean of the residuals is zero) divided by $n$.

Formally: $\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n e_i^2$, where $e_i$ is the residual, not the error.

- It can be shown (though it won't be detailed here) that $\mathbb{E}[\hat{\sigma}^2] = \frac{n-2}{n}\sigma^2$. Thus, it is biased. The unbiased estimator is $\hat{\sigma}^2 = \frac{n}{n-2}\frac{1}{n}\sum_{i=1}^n e_i^2$.

- However, more is needed. Fortunately, with the assumption of normal errors, it can be demonstrated that:

$$
\frac{n\hat{\sigma}^2}{\sigma^2} \sim \chi^2_{n-2}.
$$

---

## Standard Error of $\hat{\beta}$

- We know that $se(\hat{\beta}) = \frac{\sigma}{S_x\sqrt{n}}$.
- However, we don't know the value of $\sigma$, so we can't compute the standard errors directly.
- We can estimate the standard errors using the plug-in unbiased estimator $\frac{n-2}{n}\hat{\sigma}$ as a replacement for $\sigma$. Therefore, our estimated standard error is:

$$
\hat{se}(\hat{\beta}) = \frac{\hat{\sigma}}{S_x\sqrt{n-2}}
$$

---

## Sampling Distribution of the New Standard Error

Now we need to determine the sampling distribution of the new $\hat{\beta}$.

Using our old standard error, it's easy to show that:
$$
\frac{\hat{\beta} - \beta}{se(\hat{\beta})} \sim N(0,1)
$$

---

## Sampling Distribution of $\hat{se}(\hat{\beta})$

If we knew the true value of the variance $\sigma^2$, we could show:

$$
\begin{align}
P(\beta - 1.96 \times se(\hat{\beta}) < \hat{\beta} < \beta  + 1.96 \times se(\hat{\beta})) \\
= P(-1.96 \times se(\hat{\beta}) < \hat{\beta} - \beta <  + 1.96 \times se(\hat{\beta})) \\
= P(-1.96 < \frac{\hat{\beta} - \beta}{se(\hat{\beta})} < 1.96) \\
= \Phi(-1.96) - \Phi(1.96) = 0.95
\end{align}
$$

- Without divine intervention, we need to use $\hat{se}(\hat{\beta})$ instead.

---

## Sampling Distribution of $\hat{se}(\hat{\beta})$ (cont.)

We'll use a fact (which I won't prove) now.

If $Z \sim N(0,1)$, $S^2 \sim \chi^2_d$ and $Z$ and $S^2$ are independent, then:
$$
\frac{Z}{\sqrt{S^2/d}} \sim t_d
$$

---

## Sampling Dist. of $\hat{se}(\hat{\beta})$ (cont.2)
Applying this fact, we have:
<style>
.small-font {
    font-size: 80%;
}
</style>

<div class="small-font">
$$
\begin{align}
\frac{\hat{\beta} - \beta}{\hat{se}(\hat{\beta})} = \frac{\hat{\beta} - \beta}{\sigma} \times \frac{\sigma}{\hat{se}(\hat{\beta})} \\
= \frac{\frac{\hat{\beta} - \beta}{\sigma}}{\frac{\hat{se}(\hat{\beta})}{\sigma}} = \frac{N(0,1/ns^2_x)}{\frac{\hat{\sigma}}{\sigma S_x\sqrt{n-2}}} \\
= \frac{S_x \times N(0,1/ns^2_x)}{\frac{\hat{\sigma}}{\sigma \sqrt{n-2}}} = \frac{N(0,1/n)}{\frac{\hat{\sigma}}{\sigma \sqrt{n-2}}} \\
= \frac{\sqrt{n} \times N(0,1/n)}{\frac{\sqrt{n} \times \hat{\sigma}}{\sigma \sqrt{n-2}}} = \frac{N(0,1)}{\frac{\sqrt{n\hat{\sigma}^2}}{\sqrt{\sigma^2 \times (n-2)}}} \\
= \frac{N(0,1)}{\sqrt{\frac{n\hat{\sigma}^2}{\sigma^2 \times (n-2)}}} = \frac{N(0,1)}{\sqrt{\chi^2_{n-2}/(n-2)}} \sim t_d
\end{align}
$$
<div>

---

## Hypothesis Testing

This detailed derivation illustrates why we use the $t_d$ distribution with $d$ degrees of freedom instead of the Normal distribution for hypothesis testing.

$$ 
\frac{\hat{\beta} - \beta}{\hat{se}(\hat{\beta})} \sim t_d
$$

This approach is also applicable to computing confidence intervals.

We can further explore what happens to the $t_d$ distribution as $n$ increases.

Exercise: Run a simulation to demonstrate that it converges to a Normal distribution.

---

## Hypothesis Testing and Confidence Intervals

<style>
.small-font {
    font-size: 80%;
}
</style>

<div class="small-font">

If my null hypothesis is $\beta = 0$, then under the null model, I have:

$$ 
\frac{\hat{\beta}}{\hat{se}(\hat{\beta})} \sim t_d
$$

Suppose I collect data (30 df), estimate $\hat{\beta} = 2$, and get a standard error of $0.9$. 

- Then I calculate $2/0.9 = 2.22$, which is the $t$ statistic.

- I can compute the probability of observing a value as extreme or more extreme as the t-statistic in either tail of the $t_{30}$ distribution, also known as the p-value.
- This is computed (within R) as follows: `2 * abs(1 - pt(2.22, df=30))`.
<div>




